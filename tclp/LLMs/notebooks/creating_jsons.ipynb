{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting all relevant URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_CLAUSE_URL = \"https://chancerylaneproject.org/clauses/\"\n",
    "BASE_GLOSSARY_URL = \"https://chancerylaneproject.org/glossary/\"\n",
    "BASE_GUIDE_URL = \"https://chancerylaneproject.org/guides/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_urls(base_url, filter_function, total_pages):\n",
    "    urls = []\n",
    "    page = 1\n",
    "\n",
    "    while True:\n",
    "        url = f\"{base_url}?pagenumber={page}\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        links = soup.find_all(\"a\", href=True)\n",
    "        page_urls = [\n",
    "            urljoin(base_url, link[\"href\"])\n",
    "            for link in links\n",
    "            if filter_function(link[\"href\"])\n",
    "        ]\n",
    "\n",
    "        if not page_urls:\n",
    "            break\n",
    "\n",
    "        # Avoid duplicate links\n",
    "        urls.extend(list(set(page_urls) - set(urls)))\n",
    "        print(f\"Valid URLs on page {page}: {page_urls}\")\n",
    "\n",
    "        page += 1\n",
    "\n",
    "        if page > total_pages:\n",
    "            break\n",
    "\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clause URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clause_filter = (\n",
    "    lambda href: \"/clauses/\" in href and not \"?\" in href and href != \"/clauses/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clause_urls = get_all_urls(BASE_CLAUSE_URL, clause_filter, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clause_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glossary URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glossary_filter = (\n",
    "    lambda href: \"/glossary/\" in href\n",
    "    and href != \"/glossary/\"\n",
    "    and not href.endswith(\"?\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glossary_urls = get_all_urls(BASE_GLOSSARY_URL, glossary_filter, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(glossary_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guide URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guide_filter = (\n",
    "    lambda href: \"/guides/\" in href and href != \"/guides/\" and not href.endswith(\"?\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guide_urls = get_all_urls(BASE_GUIDE_URL, guide_filter, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(guide_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining All URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_urls = clause_urls + glossary_urls + guide_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"output_jsons\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_url_content(url):\n",
    "    \"\"\"\n",
    "    Scrapes content from a URL and processes it based on the type of content (e.g., glossary term or clause).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Example logic to determine the type of page\n",
    "        if \"/glossary/\" in url:\n",
    "            post_type = \"glossary-term\"\n",
    "        elif \"/clauses/\" in url:\n",
    "            post_type = \"clause\"\n",
    "        else:\n",
    "            post_type = \"unknown\"\n",
    "\n",
    "        title = (\n",
    "            soup.find(\"title\").get_text(strip=True)\n",
    "            if soup.find(\"title\")\n",
    "            else \"No Title\"\n",
    "        )\n",
    "        cleaned_content = \"\"\n",
    "\n",
    "        if post_type == \"glossary-term\":\n",
    "            # Extract all definitions for glossary terms\n",
    "            definitions = []\n",
    "            for definition_section in soup.select(\n",
    "                \".definition-section\"\n",
    "            ):  # Replace with actual selector\n",
    "                definitions.append(definition_section.get_text(separator=\"\\n\").strip())\n",
    "            cleaned_content = \"\\n\\n\".join(definitions)\n",
    "\n",
    "        elif post_type == \"clause\":\n",
    "            # Extract clause content\n",
    "            full_content = (\n",
    "                soup.find(\"body\").get_text(separator=\"\\n\").strip()\n",
    "            )  # Replace with actual content selector\n",
    "            cleaned_content = full_content\n",
    "\n",
    "            # Remove \"Drafting Notes\" if needed\n",
    "            cleaned_content = re.sub(\n",
    "                r\"\\[\\s*Drafting note:.*?\\]\",\n",
    "                \"\",\n",
    "                cleaned_content,\n",
    "                flags=re.DOTALL | re.IGNORECASE,\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            # Fallback for unknown post types\n",
    "            content = (\n",
    "                soup.find(\"body\").get_text(separator=\"\\n\").strip()\n",
    "                if soup.find(\"body\")\n",
    "                else \"\"\n",
    "            )\n",
    "            cleaned_content = content\n",
    "\n",
    "        return {\n",
    "            \"url\": url,\n",
    "            \"post_type\": post_type,\n",
    "            \"title\": title,\n",
    "            \"content\": cleaned_content,\n",
    "        }\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to fetch URL {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_json(data, output_dir):\n",
    "    \"\"\"\n",
    "    Saves the scraped data as a JSON file.\n",
    "    \"\"\"\n",
    "    filename = f\"{data['title'][:50].replace(' ', '_').replace('/', '_')}.json\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "    print(f\"Saved JSON: {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in all_urls:\n",
    "    scraped_data = scrape_url_content(url)\n",
    "    if scraped_data:\n",
    "        save_as_json(scraped_data, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_relevant(soup):\n",
    "    jurisdiction = soup.find(\"span\", class_=\"cfc-taxonomy\").get_text(strip=True)\n",
    "    name = soup.find(\"p\", class_=\"cfc-page-header__kicker\").get_text(strip=True)\n",
    "    title = soup.find(\"h1\", class_=\"cfc-page-header__title\").get_text(strip=True)\n",
    "    subhead = soup.find(\"div\", class_=\"cfc-page-header__text\").get_text(strip=True)\n",
    "    clause_does = (\n",
    "        soup.find(\"h2\", id=\"h-what-this-clause-does\")\n",
    "        .find_next(\"p\")\n",
    "        .get_text(strip=True)\n",
    "    )\n",
    "    clauses_div = soup.find(\n",
    "        \"div\", class_=\"cfc-pattern-clause-callout has-navy-4-background-color\"\n",
    "    )\n",
    "    clauses_text = clauses_div.get_text(separator=\"\\n\").split(\"Download this clause\")[0]\n",
    "    definitions_heading = soup.find(\"h2\", id=\"h-definitions\")\n",
    "    definitions_section = definitions_heading.find_all_next(\n",
    "        [\"p\", \"ul\", \"li\"], limit=50\n",
    "    )  # Increase limit as necessary\n",
    "    definitions_text = \"\\n\".join(\n",
    "        [element.get_text(strip=True) for element in definitions_section]\n",
    "    )\n",
    "    definitions_text = definitions_text.split(\"We'd like to hear\")[0]\n",
    "    return {\n",
    "        \"jurisdiction\": jurisdiction,\n",
    "        \"name\": name,\n",
    "        \"title\": title,\n",
    "        \"subhead\": subhead,\n",
    "        \"clause summary\": clause_does,\n",
    "        \"clause text\": clauses_text,\n",
    "        \"definitions text\": definitions_text,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in all_urls:\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    data = scrape_relevant(soup)\n",
    "    print(json.dumps(data, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TCLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
